{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and optimization\n",
    "\n",
    "In the steps of machine learning you:\n",
    "1. take data\n",
    "2. build a model\n",
    "3. evaluate a model\n",
    "4. optimize it\n",
    "5. rebuild if needed\n",
    "Then lookst at new data and the prediction\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Usually the main goal is to make accurate predictions. One measure of performance on a model is how well tha t5model will perform on new data. \n",
    "\n",
    "#### Overfitting & model optimism\n",
    "\n",
    "#### Cross-validation\n",
    "\n",
    "holdout method because a random subset of the training data is held out from the training process. So you use a training subset to fit the model and only the testing subset to evaluate the accuracy of the model. \n",
    "\n",
    "Leave out 20 - 40$ of the data as the testing subset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make up some data\n",
    "features = rand(100,5)\n",
    "target = rand(100) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "#holdout method\n",
    "N = features.shape[0]\n",
    "print(N)\n",
    "N_train = int(floor(0.7 * N))\n",
    "print(N_train)\n",
    "#do this step if you want to randomize the index\n",
    "idx = random.permutation(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split to training/test\n",
    "idx_train = idx[:N_train]\n",
    "idx_test = idx[N_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into subsets\n",
    "features_train = features[idx_train,:]\n",
    "target_train = target[idx_train]\n",
    "features_test = features[idx_test,:]\n",
    "target_test = target[idx_test]\n",
    "\n",
    "#Then do rest of the machine learning stuff i.e.\n",
    "#model = train(features_train, target_train)\n",
    "#preds_test = predict(model, features_test)\n",
    "#accuracy = evaluate_accuracy(preds_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-fold cross-validation\n",
    "\n",
    "just like holdout you have to split subsets of training data during the learning process. The difference is k-old begins by randomly splitting the data into k disjoint subsets, called folds. Typically (5, 10, 20)\n",
    "\n",
    "The predictions are aggregated after all the cycles have been completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#examle\n",
    "N = features.shape[0]\n",
    "K = 10 #number of folds you want\n",
    "\n",
    "preds_kfold = np.empty(N)\n",
    "folds = np.random.randint(0, K, size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 36 82 88 99]\n",
      "[ 4 17 46 53 54 58 63 65 69 74]\n",
      "[ 5  7 15 22 24 26 27 28 29 37 42 50 55 67 89]\n",
      "[ 9 21 31 38 40 41 43 56 59 66 70 71 73 93 96]\n",
      "[ 1 25 30 75 77 84 87 94]\n",
      "[ 2 12 23 47 49 51 83 85 86 98]\n",
      "[ 6 16 18 20 32 35 39 45 48 68 72 78 80 90]\n",
      "[11 33 62 76 81 92 95 97]\n",
      "[ 8 10 34 44 52 60 79 91]\n",
      "[ 3 13 14 19 57 61 64]\n"
     ]
    }
   ],
   "source": [
    "for idx in np.arange(K):\n",
    "    #each fold break the data into training and testin gsubsets\n",
    "    features_train = features[folds != idx, :]\n",
    "    target_train = target[folds != idx]\n",
    "    features_test = features[folds == idx, :]\n",
    "    \n",
    "    print(nonzero(folds == idx)[0])\n",
    "    \n",
    "    #build and predict\n",
    "    #model = train(features_train, target_train)\n",
    "    #preds_kfold[folds == idx] = predict(model, features_test)\n",
    "#accuracy = evaluate_acc(preds_kfold, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation\n",
    "\n",
    "Gives you a way to estimate how accurately your ML models will predict when deployed in the wild. Things to look out for\n",
    "\n",
    "1. CV methods assume that the training data forms a representative smaple from the population of interest. If you plan to deploy te modle to predict on new data, that data should be well represented by the training data. if not, the cross-validation error estimates may be overly optimistic for the error rates on future data. Solution: Ensure that any potential biases in the training data are addressed and minimized. \n",
    "2. Some datasets use features that are temporal-- for instance, using last month's revenue to forecast this month's revenue. If this is the case with your data, you must ensure that features that are available in the future can never be used to predict the past. Solution: you can structure your cross-validation holdout set or k-folds so that all the training set data is collected previous to the testing set. \n",
    "The larger the number of folds used in k-fold the better the error estimates will be, but the longer your program will take to run. Solution: use at least 10 folds (or more) when you can. For modles that train and predict quickly, you can use leave-one-out CV (k = number of data instances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
