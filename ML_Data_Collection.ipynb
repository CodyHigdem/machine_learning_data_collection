{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "A few questions we need to ask. \n",
    "\n",
    "Which input features should I include?\n",
    "How do I obtain known values of my target(label) variable?\n",
    "How much training data do I need?\n",
    "How do I know if my training data is good enough?\n",
    "\n",
    "## Which features should be included\n",
    "\n",
    "Most problems will have a ton of potential features to choose from. It may be difficult and this is why it's important to have domain knowledge. However without domain knowledge using intuition may be the next best thing. Regardless of choices a few things must be established. \n",
    "\n",
    "1. The value of the feature must be known at the time predictions are needed (for example, at the beginning of the month for a saas churn example)\n",
    "2. The feature must be numerical or categorical in nature or you'll be required to do additional feature engineering\n",
    "\n",
    "Include features only as they are believed to be valueable and absolutely needed. The more features the bigger the challenge at times. This is because it creates more noise (perturbations) as opposed to establishing the proper signal (true data relationship). So for example a unique id may not be valuable to predict SaaS churn but monthly logins may.\n",
    "\n",
    "The fewer the features the better. \n",
    "\n",
    "The more uninformative features are present, the lower the signal-to-noise ratio and thus the less accurate on average the ML model will be. \n",
    "\n",
    "Excluding can reduce accuracy as well because the model doesn't know about the neglected features, which may be predictive of the target. \n",
    "\n",
    "### Trade off practical approaches\n",
    "\n",
    "1. include all features that you suspect to be predictive of the target variable. Fit a ML model. If the accuracy of the model is sufficient, stop.\n",
    "2. Otherwise, expand the feature set by including other features that are less obviously related to the target. Fit another model and assess the accuracy. If performance is sufficient, stop.\n",
    "3. Otherwise, starting from the expanded feature set, run a ML feature selection algorithm to choose the best, most predictive subset of your expanded feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achieving Ground Truth\n",
    "\n",
    "Is a painful process especially in 'big data'. Newer projects requires waiting until there is enough data to begin training while others such as tweet sentiment means using groups of individuals or manually interpretting sentiment. \n",
    "\n",
    "Most efforts to obtain high veracity are labor intensive and while the payoff may be well worth it careful consideration of budget and time should be assessed before jumping headlong into a ML project.\n",
    "\n",
    "### How much training data is needed?\n",
    "\n",
    "A complicated question that with time you'll no doubt start getting the hang of how much you should need. Some factors that can determine training data needs include:\n",
    "\n",
    "1. complexity of the problem. Is there a simple pattern or is it nonlinear and complex\n",
    "2. Accuracy requirements. a 60% success rate required less data than a 95%.\n",
    "3. Dimensionality of the feature space. Less data is required for 2 features as opposed to 2,000\n",
    "\n",
    "As training sets grows the models on average become more accurate (the unreasonable effectiveness of data)\n",
    "\n",
    "In the churn example. Assuming 3333 instances with 19 features and an outcome of subscribed or unsubscribed the following is a way to potentially see if there is a need for more data. \n",
    "\n",
    "1. Using the current training set, choose a grid of subsample sizes to try. example sets of 500, 1000, 1500, 2500 or 3k\n",
    "2. For eac sample size, randomly draw that many instances (without replacement) from the training set.\n",
    "3. With each subsample of training data, build an ML model and access the accuracy of that model (evaluation metrics)\n",
    "4. assess hwo the accuracy changes as a function of sample size. If it seems to level off at the higher sample sizes, the existing training set is probably sufficient. But if the accuracy continues to rise for the larger samples, the inclusion of more training instances would likely boost accuracy\n",
    "\n",
    "### Is the training data representative enough?\n",
    "\n",
    "In other words how similiar are the instances in the training set to the instances that will be collected in the future? In a supervicsed machine learning model the goal is to generate accurate predictiosn on new data. If not then we will fall subject to 'sample-selection bias' or the 'covariate shift'.\n",
    "\n",
    "##### Nonrepresentative reasons\n",
    "\n",
    "1. It was possible to obtain ground truth only for a certain subsample. Credit Card fraud over a certain amount of money. Thus if the threshold of fraud was 1k. Anything below that would be difficult to detect in a supervised learning situation.\n",
    "2. The properties of the instances have changed over time. For example, if your training example consists of historical data on medical insurance fraud, but new laws have substatially changed the ways in which medical insurers must conduct their business, then your predictions on the new data may not be appropriate\n",
    "3. The input feature set has changed over time. Say the set of location attributes that you collect on each customer has changed; you used to collect ZIP code and state but now you collect IP address. This change may require you to modify the feature set used for the model and potentially discard old data from the training set. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#convert categorical features to numerical binary features example\n",
    "\n",
    "def cat_to_num(data):\n",
    "    data = np.array(data)\n",
    "    categories = np.unique(data)\n",
    "    features = []\n",
    "    for cat in categories:\n",
    "        binary = (data == cat)\n",
    "        features.append(binary.astype('int'))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0]\n",
      "[1 0 1 0 1]\n",
      "[array([0, 1, 0, 1, 0]), array([1, 0, 1, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "gender = ['male', 'female', 'male', 'female','male']\n",
    "gender_cat = cat_to_num(gender)\n",
    "print(gender_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
