{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "A few questions we need to ask. \n",
    "\n",
    "Which input features should I include?\n",
    "How do I obtain known values of my target(label) variable?\n",
    "How much training data do I need?\n",
    "How do I know if my training data is good enough?\n",
    "\n",
    "## Which features should be included\n",
    "\n",
    "Most problems will have a ton of potential features to choose from. It may be difficult and this is why it's important to have domain knowledge. However without domain knowledge using intuition may be the next best thing. Regardless of choices a few things must be established. \n",
    "\n",
    "1. The value of the feature must be known at the time predictions are needed (for example, at the beginning of the month for a saas churn example)\n",
    "2. The feature must be numerical or categorical in nature or you'll be required to do additional feature engineering\n",
    "\n",
    "Include features only as they are believed to be valueable and absolutely needed. The more features the bigger the challenge at times. This is because it creates more noise (perturbations) as opposed to establishing the proper signal (true data relationship). So for example a unique id may not be valuable to predict SaaS churn but monthly logins may.\n",
    "\n",
    "The fewer the features the better. \n",
    "\n",
    "The more uninformative features are present, the lower the signal-to-noise ratio and thus the less accurate on average the ML model will be. \n",
    "\n",
    "Excluding can reduce accuracy as well because the model doesn't know about the neglected features, which may be predictive of the target. \n",
    "\n",
    "### Trade off practical approaches\n",
    "\n",
    "1. include all features that you suspect to be predictive of the target variable. Fit a ML model. If the accuracy of the model is sufficient, stop.\n",
    "2. Otherwise, expand the feature set by including other features that are less obviously related to the target. Fit another model and assess the accuracy. If performance is sufficient, stop.\n",
    "3. Otherwise, starting from the expanded feature set, run a ML feature selection algorithm to choose the best, most predictive subset of your expanded feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achieving Ground Truth\n",
    "\n",
    "Is a painful process especially in 'big data'. Newer projects requires waiting until there is enough data to begin training while others such as tweet sentiment means using groups of individuals or manually interpretting sentiment. \n",
    "\n",
    "Most efforts to obtain high veracity are labor intensive and while the payoff may be well worth it careful consideration of budget and time should be assessed before jumping headlong into a ML project.\n",
    "\n",
    "### How much training data is needed?\n",
    "\n",
    "A complicated question that with time you'll no doubt start getting the hang of how much you should need. Some factors that can determine training data needs include:\n",
    "\n",
    "1. complexity of the problem. Is there a simple pattern or is it nonlinear and complex\n",
    "2. Accuracy requirements. a 60% success rate required less data than a 95%.\n",
    "3. Dimensionality of the feature space. Less data is required for 2 features as opposed to 2,000\n",
    "\n",
    "As training sets grows the models on average become more accurate (the unreasonable effectiveness of data)\n",
    "\n",
    "In the churn example. Assuming 3333 instances with 19 features and an outcome of subscribed or unsubscribed the following is a way to potentially see if there is a need for more data. \n",
    "\n",
    "1. Using the current training set, choose a grid of subsample sizes to try. example sets of 500, 1000, 1500, 2500 or 3k\n",
    "2. For eac sample size, randomly draw that many instances (without replacement) from the training set.\n",
    "3. With each subsample of training data, build an ML model and access the accuracy of that model (evaluation metrics)\n",
    "4. assess hwo the accuracy changes as a function of sample size. If it seems to level off at the higher sample sizes, the existing training set is probably sufficient. But if the accuracy continues to rise for the larger samples, the inclusion of more training instances would likely boost accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
