{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Prediction\n",
    "\n",
    "The role of machine learning is to discover patterns and relationship in data and to put those use. The end goal is pretty simple though the methods may range from simple to extremely complicated, the goal however is to take the known and predict what we don't know. \n",
    "\n",
    "### Finding relationships between input and target (label) data\n",
    "\n",
    "Trying to predict MPG for vehicles. The dataset could have model year, vehicle weight, horsepower, number of cylinders so on and the vehicles MPG rating. \n",
    "\n",
    "Input features are typically referred to using the symbol X, with subscripts differenting inputs when multiple input features exist. For example X_1 refers to manufactuer region, X_2 to model year and so on. \n",
    "\n",
    "Target variable is typically referred as Y. So a simple formula could be\n",
    "\n",
    "Y = f(X) + Error\n",
    "\n",
    "f: is the unknown function that relates the input variables to the target, Y. It is commonly referred to as the signal. \n",
    "\n",
    "E:(error) is called noise.\n",
    "\n",
    "##### \"The challenge of machine learning is to use data to determine what the true signal is, while ignoring noise\"\n",
    "\n",
    "If you knew f() in the auto mobile challenge then you'd be able to know the MPG rating of any car. But you could have numerous sources of noise, E, including:\n",
    "\n",
    "1. imperfect measurement of each vehicle's MPG rating cased by small inaccuracies in the measuring devices --measurement noise\n",
    "2. Variations in manufacturing process, causing each care in the fleet to have slightly different MPG measurements --- manufacturing process noise\n",
    "3. Noise in the measurement of the input features, such as weight and horsepower\n",
    "4. Lack of access to the braoder set of features that would exactly determine MPG\n",
    "\n",
    "Assuming a good estimate of f. Machine learning has 2 goals predictions and inference. \n",
    "\n",
    "#### Prediction\n",
    "\n",
    "Giving a healthymodel you can generate predictions of the target (Y) given new information (X_new). Giving you new data as needed. \n",
    "\n",
    "Examples of ML prediction cases:\n",
    "deciphering handwritten digits or voice recordings\n",
    "predicting stock market\n",
    "forecasting\n",
    "predicting which users are most likely to click, convert or buy\n",
    "predicting which users will need product support and which are liekly to unsubscribe\n",
    "determining which transactions are fraudulent\n",
    "making recommendations\n",
    "\n",
    "#### Inference\n",
    "\n",
    "machine learning models and better understand the relationships between theinput features and the output target. Such as:\n",
    "\n",
    "which input features are most strongly related to the target variable?\n",
    "Are those relationships positive or negative?\n",
    "Is f a simple relationship, or is it a function that's more nuanced and nonlinear?\n",
    "\n",
    "## Models\n",
    "\n",
    "#### Parametric vs nonparametric\n",
    "assume that f takes a specific functional form, whereas nonparametric models don't make such strict assumptions. Parametric approaches tend to be simple and interpretable, but less accurate. \n",
    "\n",
    "Nonparametric approaches are usually les interpretable but more accurate across a broad range of problems. \n",
    "\n",
    "### Parametric Methods\n",
    "\n",
    "linear regression is a parametric models. It assumes f is a linear combination of the numerical values of the inputs. \n",
    "\n",
    "f(X) = B_0 + X_1 x B_1 + X_2 x B_2\n",
    "\n",
    "other commonly used parametric models include:\n",
    "\n",
    "logistic regression\n",
    "polynomial regression\n",
    "linear discriminant analysis\n",
    "quadratic discriminant analysis\n",
    "(parametric) mixture models\n",
    "naive bayes\n",
    "\n",
    "#### drawbacks\n",
    "\n",
    "the biggest drawback is the strong assumption about the true form of the function\n",
    "\n",
    "### Nonparametric methods\n",
    "\n",
    "f doesn't take a simple fixed function. The form and complexity of f adapts to complexity of the data. For example a classification tree.\n",
    "\n",
    "Other examples:\n",
    "k-nearest neighbors\n",
    "splines\n",
    "basis expansion methods\n",
    "kernal smoothing\n",
    "generalized additive models\n",
    "neural nets\n",
    "bagging\n",
    "boosting\n",
    "random forests\n",
    "support vector machines\n",
    "\n",
    "\n",
    "### supervised vs unsupervised\n",
    "\n",
    "supervised problems is where you have access to the target variable for set of training data. Unsupervised are ones in which there's no identified target variable.\n",
    "\n",
    "unsupervised have 2 main classes:\n",
    "\n",
    "#### clustering\n",
    "\n",
    "use the inpute features to discover natural groupings (k-means so on)\n",
    "\n",
    "#### dimensionality reduction\n",
    "transform the input features into a small number of coordinates that caputre most of the variability of the data (principle component analysis (PCA), multidimensional scaling, manifold learning\n",
    "\n",
    "### Classifications\n",
    "\n",
    "this is about putting things into buckets so to speak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example code\n",
    "from sklearn.linear_model import LogisticRegression as Model\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(features, target):\n",
    "    print(features)\n",
    "    print(target)\n",
    "    model = Model()\n",
    "    model.fit(features, target)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, new_features):\n",
    "    preds = model.predict(new_features)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_feats = np.array([[3,1,3], [22, 38, 25]])\n",
    "titanic_target = np.array([0,1,0])\n",
    "titanic_test = np.array([3,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  1  3]\n",
      " [22 38 25]]\n",
      "[0 1 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f7c4147bf871>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitanic_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitanic_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#predictions = predict(model, titanic_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-98d18457ce1d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(features, target)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 3]"
     ]
    }
   ],
   "source": [
    "model = train(titanic_feats, titanic_target)\n",
    "#predictions = predict(model, titanic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
