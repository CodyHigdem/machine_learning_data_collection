{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv(\"data/titanic.csv\")\n",
    "#check if data loaded by grabbing first 5\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Prediction\n",
    "\n",
    "The role of machine learning is to discover patterns and relationship in data and to put those use. The end goal is pretty simple though the methods may range from simple to extremely complicated, the goal however is to take the known and predict what we don't know. \n",
    "\n",
    "### Finding relationships between input and target (label) data\n",
    "\n",
    "Trying to predict MPG for vehicles. The dataset could have model year, vehicle weight, horsepower, number of cylinders so on and the vehicles MPG rating. \n",
    "\n",
    "Input features are typically referred to using the symbol X, with subscripts differenting inputs when multiple input features exist. For example X_1 refers to manufactuer region, X_2 to model year and so on. \n",
    "\n",
    "Target variable is typically referred as Y. So a simple formula could be\n",
    "\n",
    "Y = f(X) + Error\n",
    "\n",
    "f: is the unknown function that relates the input variables to the target, Y. It is commonly referred to as the signal. \n",
    "\n",
    "E:(error) is called noise.\n",
    "\n",
    "##### \"The challenge of machine learning is to use data to determine what the true signal is, while ignoring noise\"\n",
    "\n",
    "If you knew f() in the auto mobile challenge then you'd be able to know the MPG rating of any car. But you could have numerous sources of noise, E, including:\n",
    "\n",
    "1. imperfect measurement of each vehicle's MPG rating cased by small inaccuracies in the measuring devices --measurement noise\n",
    "2. Variations in manufacturing process, causing each care in the fleet to have slightly different MPG measurements --- manufacturing process noise\n",
    "3. Noise in the measurement of the input features, such as weight and horsepower\n",
    "4. Lack of access to the braoder set of features that would exactly determine MPG\n",
    "\n",
    "Assuming a good estimate of f. Machine learning has 2 goals predictions and inference. \n",
    "\n",
    "#### Prediction\n",
    "\n",
    "Giving a healthymodel you can generate predictions of the target (Y) given new information (X_new). Giving you new data as needed. \n",
    "\n",
    "Examples of ML prediction cases:\n",
    "deciphering handwritten digits or voice recordings\n",
    "predicting stock market\n",
    "forecasting\n",
    "predicting which users are most likely to click, convert or buy\n",
    "predicting which users will need product support and which are liekly to unsubscribe\n",
    "determining which transactions are fraudulent\n",
    "making recommendations\n",
    "\n",
    "#### Inference\n",
    "\n",
    "machine learning models and better understand the relationships between theinput features and the output target. Such as:\n",
    "\n",
    "which input features are most strongly related to the target variable?\n",
    "Are those relationships positive or negative?\n",
    "Is f a simple relationship, or is it a function that's more nuanced and nonlinear?\n",
    "\n",
    "## Models\n",
    "\n",
    "#### Parametric vs nonparametric\n",
    "assume that f takes a specific functional form, whereas nonparametric models don't make such strict assumptions. Parametric approaches tend to be simple and interpretable, but less accurate. \n",
    "\n",
    "Nonparametric approaches are usually les interpretable but more accurate across a broad range of problems. \n",
    "\n",
    "### Parametric Methods\n",
    "\n",
    "linear regression is a parametric models. It assumes f is a linear combination of the numerical values of the inputs. \n",
    "\n",
    "f(X) = B_0 + X_1 x B_1 + X_2 x B_2\n",
    "\n",
    "other commonly used parametric models include:\n",
    "\n",
    "logistic regression\n",
    "polynomial regression\n",
    "linear discriminant analysis\n",
    "quadratic discriminant analysis\n",
    "(parametric) mixture models\n",
    "naive bayes\n",
    "\n",
    "#### drawbacks\n",
    "\n",
    "the biggest drawback is the strong assumption about the true form of the function\n",
    "\n",
    "### Nonparametric methods\n",
    "\n",
    "f doesn't take a simple fixed function. The form and complexity of f adapts to complexity of the data. For example a classification tree.\n",
    "\n",
    "Other examples:\n",
    "k-nearest neighbors\n",
    "splines\n",
    "basis expansion methods\n",
    "kernal smoothing\n",
    "generalized additive models\n",
    "neural nets\n",
    "bagging\n",
    "boosting\n",
    "random forests\n",
    "support vector machines\n",
    "\n",
    "\n",
    "### supervised vs unsupervised\n",
    "\n",
    "supervised problems is where you have access to the target variable for set of training data. Unsupervised are ones in which there's no identified target variable.\n",
    "\n",
    "unsupervised have 2 main classes:\n",
    "\n",
    "#### clustering\n",
    "\n",
    "use the inpute features to discover natural groupings (k-means so on)\n",
    "\n",
    "#### dimensionality reduction\n",
    "transform the input features into a small number of coordinates that caputre most of the variability of the data (principle component analysis (PCA), multidimensional scaling, manifold learning\n",
    "\n",
    "### Classifications\n",
    "\n",
    "this is about putting things into buckets so to speak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example code\n",
    "from sklearn.linear_model import LogisticRegression as Model\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make 80/20 train test split\n",
    "data_train = data[:int(0.8*len(data))]\n",
    "data_test = data[int(0.8*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_num(data):\n",
    "    categories = unique(data)\n",
    "    features = {}\n",
    "    for cat in categories:\n",
    "        binary = (data == cat)\n",
    "        features[\"%s=%s\" % (data.name, cat)] = binary.astype(\"int\")\n",
    "    return pandas.DataFrame(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop([\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n",
    "    \n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "    \n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = sqrt(data[\"Fare\"])\n",
    "    \n",
    "    # Adding gender categorical value\n",
    "    features = features.join( cat_to_num(data['Sex']) )\n",
    "    \n",
    "\n",
    "    # Adding Embarked categorical value\n",
    "    \n",
    "    #features = features.join( cat_to_num(data['Embarked']) )\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building logistical regression classifier via scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>sqrt_Fare</th>\n",
       "      <th>Sex=female</th>\n",
       "      <th>Sex=male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.442944</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.815138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.286975</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.837252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch  sqrt_Fare  Sex=female  Sex=male\n",
       "0       3  22.0      1      0   2.692582           0         1\n",
       "1       1  38.0      1      0   8.442944           1         0\n",
       "2       3  26.0      0      0   2.815138           1         0\n",
       "3       1  35.0      1      0   7.286975           1         0\n",
       "4       3  35.0      0      0   2.837252           0         1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the features\n",
    "features = prepare_data(data_train)\n",
    "#see if it worked with the first 5 entries\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(features, target):\n",
    "    print(features)\n",
    "    print(target)\n",
    "    model = Model()\n",
    "    model.fit(features, target)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, new_features):\n",
    "    preds = model.predict(new_features)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.fit(features, data_train[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(prepare_data(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82122905027932958"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what is the accuracy\n",
    "model.score(prepare_data(data_test), data_test[\"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Is the simplest ML algo for classification problems. To understand think of a problem as only having to features and a dataset divided into 2 classes. Such as the titanic features of Age and sqft(Fare). The target(label) data of survived or died. \n",
    "\n",
    "The Algo consists of the following steps:\n",
    "1. You can start the search by picking the parameter values at random, hence lacing a random line in an x,y chart for visualization\n",
    "2. Measure how well this line separates the 2 classes. In logistic regression, you use the statistical deviances for the 'goodness-of-fit' measurement. \n",
    "3. Guess new values of the parameters and measure the separation power.\n",
    "4. Repeat until there are no better guesses. This is an optimization procedure that can be done with a range of optimization algorithms. Gradient descent is a popular choice for a simple optimization algo.\n",
    "\n",
    "This approach can be extended to more dimensions, so you're not limited to 2 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
